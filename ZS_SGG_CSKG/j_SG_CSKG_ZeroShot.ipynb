{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSKG Setup, Installation and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Clone repos\n",
    "os.chdir('ZS_SGG_CSKG/')\n",
    "if os.path.isdir('cskg')==False:\n",
    "    !git clone https://github.com/usc-isi-i2/cskg\n",
    "os.chdir('cskg')\n",
    "if os.path.isdir('grounding/graphify')==False:\n",
    "    !git clone https://github.com/ucinlp/mowgli-uci\n",
    "    !mv mowgli-uci/* grounding/\n",
    "# Remove versions from packge names in dependencies to minimize conflict\n",
    "if os.path.isfile('requirements1.txt')==False:\n",
    "    file2 = open(\"requirements1.txt\",\"w\")\n",
    "    with open(\"requirements.txt\", \"r\") as file1:\n",
    "        for pkgver in file1:\n",
    "            if 'kgtk' in pkgver:\n",
    "                continue\n",
    "            if 'demjson' in pkgver:\n",
    "                continue\n",
    "            if '==' in pkgver:\n",
    "                [pkg, ver] = pkgver.split('==')\n",
    "                file2.write(pkg+'\\n')\n",
    "            if ' @ ' in pkgver:\n",
    "                [pkg, ver] = pkgver.split(' @ ')\n",
    "                file2.write(pkg+'\\n')\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "\n",
    "if os.path.isfile('grounding/requirements1.txt')==False:\n",
    "    file2 = open(\"grounding/requirements1.txt\",\"w\")\n",
    "    with open(\"grounding/requirements.txt\", \"r\") as file1:\n",
    "        for pkgver in file1:\n",
    "            if '==' in pkgver:\n",
    "                [pkg, ver] = pkgver.split('==')\n",
    "                file2.write(pkg+'\\n')\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools==57.5.0\n",
    "!pip install --upgrade python-pushover\n",
    "!python -m pip install kgtk==0.5.0\n",
    "!python -m pip install -r requirements1.txt --no-cache-dir\n",
    "!python -m pip install -r grounding/requirements1.txt --no-cache-dir\n",
    "!conda install --yes faiss-cpu -c pytorch #-n mowgli\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m pip install kgtk==0.5.0\n",
    "!python -m pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGTK 0.5.0\n",
      "usage: kgtk query [-h] -i INPUT_FILE [INPUT_FILE ...] [--as NAME]\n",
      "                  [--query QUERY] [--match PATTERN] [--where CLAUSE]\n",
      "                  [--return CLAUSE] [--order-by CLAUSE] [--skip CLAUSE]\n",
      "                  [--limit CLAUSE] [--para NAME=VAL] [--spara NAME=VAL]\n",
      "                  [--lqpara NAME=VAL] [--no-header] [--index [MODE]]\n",
      "                  [--explain [MODE]] [--graph-cache GRAPH_CACHE_FILE]\n",
      "                  [-o OUTPUT]\n",
      "\n",
      "Query one or more KGTK files with Kypher.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -i INPUT_FILE [INPUT_FILE ...], --input-files INPUT_FILE [INPUT_FILE ...]\n",
      "                        One or more input files to query (maybe compressed).\n",
      "                        (Required, use '-' for stdin.)\n",
      "  --as NAME             alias name to be used for preceding input\n",
      "  --query QUERY         complete Kypher query combining all clauses, if\n",
      "                        supplied, all other specialized clause arguments will\n",
      "                        be ignored\n",
      "  --match PATTERN       MATCH pattern of a Kypher query, defaults to universal\n",
      "                        node pattern `()'\n",
      "  --where CLAUSE        WHERE clause of a Kypher query\n",
      "  --return CLAUSE       RETURN clause of a Kypher query (defaults to *)\n",
      "  --order-by CLAUSE     ORDER BY clause of a Kypher query\n",
      "  --skip CLAUSE         SKIP clause of a Kypher query\n",
      "  --limit CLAUSE        LIMIT clause of a Kypher query\n",
      "  --para NAME=VAL       zero or more named value parameters to be passed to\n",
      "                        the query\n",
      "  --spara NAME=VAL      zero or more named string parameters to be passed to\n",
      "                        the query\n",
      "  --lqpara NAME=VAL     zero or more named LQ-string parameters to be passed\n",
      "                        to the query\n",
      "  --no-header           do not generate a header row with column names\n",
      "  --index [MODE]        control column index creation according to MODE (auto,\n",
      "                        expert, quad, triple, node1+label, node1, label,\n",
      "                        node2, none, default: auto)\n",
      "  --explain [MODE]      explain the query execution and indexing plan\n",
      "                        according to MODE (plan, full, expert, default: plan).\n",
      "                        This will not actually run or create anything.\n",
      "  --graph-cache GRAPH_CACHE_FILE\n",
      "                        database cache where graphs will be imported before\n",
      "                        they are queried (defaults to per-user temporary file)\n",
      "  -o OUTPUT, --out OUTPUT\n",
      "                        output file to write to, if `-' (the default) output\n",
      "                        goes to stdout. Files with extensions .gz, .bz2 or .xz\n",
      "                        will be appopriately compressed.\n"
     ]
    }
   ],
   "source": [
    "!kgtk --version\n",
    "!kgtk query -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads:\n",
    "# 1) download all files from https://drive.google.com/drive/u/1/folders/16347KHSloJJZIbgC9V5gH7_pRx0CzjPQ \n",
    "# and place in 'cskg/output' folder, and unzip all\n",
    "# 2) move BERT embeddings ('bert_nli_large_w2v_format.txt.gz') to 'cskg/output/embeddings'\n",
    "# 3) download numberbatch\n",
    "# !wget https://conceptnet.s3.amazonaws.com/downloads/2019/numberbatch/numberbatch-19.08.txt.gz -P output/embeddings/\n",
    "\n",
    "# Unzip Downloads:\n",
    "#!gunzip -k output/*.txt.gz\n",
    "#!gunzip -k output/*.tsv.gz\n",
    "#!gunzip -k output/embeddings/*.txt.gz\n",
    "\n",
    "# Run these in terminal:\n",
    "# sudo apt-get update -y\n",
    "# sudo apt-get upgrade -y\n",
    "# sudo apt-get install -y graphviz\n",
    "# python -m pip install graphviz\n",
    "# sudo apt-get install -y xdg-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ZS_SGG_CSKG/cskg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cskg_path = \"output\" #\"../output\" \n",
    "kg = \"cskg_connected.kgtk.gz\" #kg = \"cskg.tsv.gz\"\n",
    "nkg = \"cskg-normalized.kgtk.gz\"\n",
    "delete_database = \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ['CSKG'] = cskg_path\n",
    "os.environ['KG'] = \"{}/{}\".format(cskg_path, kg)\n",
    "os.environ['NKG'] = \"{}/{}\".format(cskg_path, nkg)\n",
    "os.environ['STORE'] = \"{}/wikidata.sqlite3.db\".format(cskg_path)\n",
    "os.environ['kypher'] = \"kgtk query --graph-cache \" + os.environ['STORE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bar_chart(data, x_column, y_column, title=\"\", width=800):\n",
    "    \"\"\"Construct a simple bar chart with two properties\"\"\"\n",
    "    bars = alt.Chart(data).mark_bar().encode(\n",
    "        y=alt.Y(y_column, sort='-x'),\n",
    "        x=x_column\n",
    "    ).properties(\n",
    "        title=title,\n",
    "        width=width\n",
    "    )\n",
    "\n",
    "    text = bars.mark_text(\n",
    "        align='left',\n",
    "        baseline='middle',\n",
    "        dx=3  # Nudges text to right so it doesn't appear on top of the bar\n",
    "    ).encode(\n",
    "        text=x_column\n",
    "    )\n",
    "\n",
    "    return (bars + text)\n",
    "\n",
    "import io\n",
    "import pandas\n",
    "import subprocess\n",
    "\n",
    "def shell_df(command, shell=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Takes a shell command as a string and and reads the result into a Pandas DataFrame.\n",
    "    \n",
    "    Additional keyword arguments are passed through to pandas.read_csv.\n",
    "    \n",
    "    :param command: a shell command that returns tabular data\n",
    "    :type command: str\n",
    "    :param shell: passed to subprocess.Popen\n",
    "    :type shell: bool\n",
    "    \n",
    "    :return: a pandas dataframe\n",
    "    :rtype: :class:`pandas.dataframe`\n",
    "    \"\"\"\n",
    "    proc = subprocess.Popen(command, \n",
    "                            shell=shell,\n",
    "                            stdout=subprocess.PIPE, \n",
    "                            stderr=subprocess.PIPE)\n",
    "    output, error = proc.communicate()\n",
    "    \n",
    "    if proc.returncode == 0:\n",
    "        if error:\n",
    "            print(error.decode())\n",
    "        with io.StringIO(output.decode()) as buffer:\n",
    "            return pandas.read_csv(buffer, **kwargs)\n",
    "    else:\n",
    "        message = (\"Shell command returned non-zero exit status: {0}\\n\\n\"\n",
    "                   \"Command was:\\n{1}\\n\\n\"\n",
    "                   \"Standard error was:\\n{2}\")\n",
    "        raise IOError(message.format(proc.returncode, command, error.decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize the file so that it is easier to process with Kypher\n",
    "#!kgtk normalize --verbose -i $KG -o $CSKG/temp.cskg.normalize.1.kgtk.gz --columns-to-lower 'relation;dimension' source sentence 'node1;label' 'relation;label' 'node2;label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename the columns to the standard node1/label/node2 and add ids\n",
    "#!kgtk rename-columns --mode NONE -i $CSKG/temp.cskg.normalize.1.kgtk.gz --output-columns id node1 label node2 \\\n",
    "#    / add-id --id-style node1-label-node2 -o $NKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSKG Embeddings\n",
    "graph_emb_trans = \"trans_log_dot_0.1.tsv.gz\" #graph embedding output file - TransE\n",
    "graph_emb_comp = \"comp_log_dot_0.1.tsv.gz\" #graph embedding output file - ComplEx\n",
    "graph_emb_dist = \"dist_log_dot_0.1.tsv.gz\" #graph embedding output file - DistMult\n",
    "graph_emb_resc = \"resc_log_dot_0.1.tsv.gz\" #graph embedding output file - RESCAL\n",
    "text_emb = \"cskg_embeddings_bert_nli_large.txt.gz\" #text embedding output file - BERT\n",
    "distance='cosine' #embedding distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Callable, List, Tuple\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\n",
      "output/trans_log_dot_0.1.tsv.gz\n",
      "output/comp_log_dot_0.1.tsv.gz\n",
      "output/dist_log_dot_0.1.tsv.gz\n",
      "output/resc_log_dot_0.1.tsv.gz\n",
      "output/cskg_embeddings_bert_nli_large.txt.gz\n"
     ]
    }
   ],
   "source": [
    "os.environ['CSKG'] = cskg_path\n",
    "os.environ['GE_trans'] = \"{}/{}\".format(cskg_path, graph_emb_trans)\n",
    "os.environ['GE_comp'] = \"{}/{}\".format(cskg_path, graph_emb_comp)\n",
    "os.environ['GE_dist'] = \"{}/{}\".format(cskg_path, graph_emb_dist)\n",
    "os.environ['GE_resc'] = \"{}/{}\".format(cskg_path, graph_emb_resc)\n",
    "os.environ['TE'] = \"{}/{}\".format(cskg_path, text_emb)\n",
    "graph_emb_trans_path = os.environ['GE_trans']\n",
    "graph_emb_comp_path = os.environ['GE_comp']\n",
    "graph_emb_dist_path = os.environ['GE_dist']\n",
    "graph_emb_resc_path = os.environ['GE_resc']\n",
    "text_emb_path = os.environ['TE']\n",
    "!echo $CSKG\n",
    "!echo $GE_trans\n",
    "!echo $GE_comp\n",
    "!echo $GE_dist\n",
    "!echo $GE_resc\n",
    "!echo $TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, words) -> None:\n",
    "        self.idx_to_word = words\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "def read_embedding_file(embedding_file: Path, dim: int, emb_col=1) -> Tuple[Vocab, np.ndarray]:\n",
    "\n",
    "    logger.debug(f'Reading embeddings from {embedding_file}')\n",
    "\n",
    "    shape = tuple([count_lines(embedding_file), dim])\n",
    "                  \n",
    "    with gzip.open(embedding_file, 'r') as f:\n",
    "\n",
    "        embeddings = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        if emb_col!=1:\n",
    "            header=next(f)\n",
    "        i=0\n",
    "        words = []\n",
    "        for line in tqdm(f, total=shape[0]):\n",
    "            line=line.decode()\n",
    "            if emb_col==1:\n",
    "                node, *embedding = line.split()\n",
    "            else:\n",
    "                line_data=line.split()\n",
    "                if line_data[1]=='embedding_sentence': continue\n",
    "                node=line_data[0]\n",
    "                embedding=line_data[2].split(',')\n",
    "            embedding = np.array([float(x) for x in embedding])\n",
    "            words.append(node)\n",
    "            embeddings[i] = embedding\n",
    "            i+=1\n",
    "\n",
    "    vocab = Vocab(words)\n",
    "\n",
    "    return vocab, embeddings\n",
    "\n",
    "def count_lines(embedding_file: Path):\n",
    "    with gzip.open(embedding_file, 'r') as f:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            i+=1\n",
    "    return i\n",
    "\n",
    "def build_index(metric: str, embeddings: np.ndarray):\n",
    "\n",
    "    logger.debug(f'Building search index')\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[-1])\n",
    "    elif metric == 'l2':\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[-1])\n",
    "    else:\n",
    "        raise ValueError(f'Bad metric: {metric}')\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2160968/2160968 [04:17<00:00, 8388.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load graph embeddings\n",
    "graph_dim = 100 # Dimension of the graph embeddings for our example's file\n",
    "#graph_vocab_trans, graph_embeddings_trans = read_embedding_file(graph_emb_trans_path,graph_dim)\n",
    "#graph_index_trans = build_index(distance, graph_embeddings_trans)\n",
    "graph_vocab_comp, graph_embeddings_comp = read_embedding_file(graph_emb_comp_path,graph_dim)\n",
    "graph_index_comp = build_index(distance, graph_embeddings_comp)\n",
    "#graph_vocab_dist, graph_embeddings_dist = read_embedding_file(graph_emb_dist_path,graph_dim)\n",
    "#graph_index_dist = build_index(distance, graph_embeddings_dist)\n",
    "#graph_vocab_resc, graph_embeddings_resc = read_embedding_file(graph_emb_resc_path,graph_dim)\n",
    "#graph_index_resc = build_index(distance, graph_embeddings_resc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load text embeddings\n",
    "#text_dim = 1024 # Dimension of the text embeddings for our example's file\n",
    "#text_vocab, text_embeddings = read_embedding_file(text_emb_path, text_dim, emb_col=2)\n",
    "#text_index = build_index(distance, text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection and Scene Graph Generation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_eval_io = 'ZS_SGG_CSKG/Eval_IO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_single_box(pic, box, color='red', draw_info=None):\n",
    "    draw = ImageDraw.Draw(pic)\n",
    "    x1,y1,x2,y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n",
    "    draw.rectangle(((x1, y1), (x2, y2)), outline=color)\n",
    "    if draw_info:\n",
    "        draw.rectangle(((x1, y1), (x1+50, y1+10)), fill=color)\n",
    "        info = draw_info\n",
    "        draw.text((x1, y1), info)\n",
    "        \n",
    "def print_list(name, input_list, scores=None):\n",
    "    for i, item in enumerate(input_list):\n",
    "        if scores == None:\n",
    "            print(name + ' ' + str(i) + ': ' + str(item))\n",
    "        else:\n",
    "            print(name + ' ' + str(i) + ': ' + str(item) + '; score: ' + str(scores[i]))\n",
    "    \n",
    "def draw_image(img_path, boxes, box_labels, rel_labels, box_scores=None, rel_scores=None):\n",
    "    size = get_size(Image.open(img_path).size)\n",
    "    pic = Image.open(img_path).resize(size)\n",
    "    num_obj = len(boxes)\n",
    "    for i in range(num_obj):\n",
    "        info = str(i) + '_' + box_labels[i]\n",
    "        draw_single_box(pic, boxes[i], draw_info=info)\n",
    "    display(pic)\n",
    "    print('*' * 50)\n",
    "    print_list('box_labels', box_labels, box_scores)\n",
    "    print('*' * 50)\n",
    "    print_list('rel_labels', rel_labels, rel_scores)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_size(image_size):\n",
    "    min_size = 600\n",
    "    max_size = 1000\n",
    "    w, h = image_size\n",
    "    size = min_size\n",
    "    if max_size is not None:\n",
    "        min_original_size = float(min((w, h)))\n",
    "        max_original_size = float(max((w, h)))\n",
    "        if max_original_size / min_original_size * size > max_size:\n",
    "            size = int(round(max_size * min_original_size / max_original_size))\n",
    "    if (w <= h and w == size) or (h <= w and h == size):\n",
    "        return (w, h)\n",
    "    if w < h:\n",
    "        ow = size\n",
    "        oh = int(size * h / w)\n",
    "    else:\n",
    "        oh = size\n",
    "        ow = int(size * w / h)\n",
    "    return (ow, oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "def load_sg(img_path, box_topk):\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    custom_prediction_file = open(f'{dir_eval_io}1_det_objs/'+img_path.split('0_images/')[1]+'/custom_prediction.json')\n",
    "    custom_prediction = json.load(custom_prediction_file)\n",
    "    custom_data_info_file = open(f'{dir_eval_io}1_det_objs/'+img_path.split('0_images/')[1]+'/custom_data_info.json')\n",
    "    custom_data_info = json.load(custom_data_info_file)\n",
    "\n",
    "    image_idx = 0\n",
    "    #box_topk = 5 # select top k bounding boxes, now taken as func arg\n",
    "    rel_topk = int(np.math.factorial(box_topk)/np.math.factorial(box_topk-2)) # select top k relationships\n",
    "    ind_to_classes = custom_data_info['ind_to_classes']\n",
    "    ind_to_predicates = custom_data_info['ind_to_predicates']\n",
    "    boxes = custom_prediction[str(image_idx)]['bbox'][:box_topk]\n",
    "    box_labels = custom_prediction[str(image_idx)]['bbox_labels'][:box_topk]\n",
    "    box_scores = custom_prediction[str(image_idx)]['bbox_scores'][:box_topk]\n",
    "    all_rel_labels = custom_prediction[str(image_idx)]['rel_labels']\n",
    "    all_rel_scores = custom_prediction[str(image_idx)]['rel_scores']\n",
    "    all_rel_pairs = custom_prediction[str(image_idx)]['rel_pairs']\n",
    "\n",
    "    for i in range(len(box_labels)):\n",
    "        box_labels[i] = ind_to_classes[box_labels[i]]\n",
    "\n",
    "    rel_labels = []\n",
    "    rel_scores = []\n",
    "    for i in range(len(all_rel_pairs)):\n",
    "        if all_rel_pairs[i][0] < box_topk and all_rel_pairs[i][1] < box_topk:\n",
    "            rel_scores.append(all_rel_scores[i])\n",
    "            label = str(all_rel_pairs[i][0]) + '_' + box_labels[all_rel_pairs[i][0]] + ' => ' + ind_to_predicates[all_rel_labels[i]] + ' => ' + str(all_rel_pairs[i][1]) + '_' + box_labels[all_rel_pairs[i][1]]\n",
    "            rel_labels.append(label)\n",
    "\n",
    "    rel_labels = rel_labels[:rel_topk]\n",
    "    rel_scores = rel_scores[:rel_topk]\n",
    "\n",
    "    #draw_image(img, boxes, box_labels, rel_labels, box_scores=box_scores, rel_scores=rel_scores)\n",
    "    \n",
    "    return box_labels,rel_labels,boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "def draw_graph(nodes,edges):\n",
    "    \n",
    "    #init graph\n",
    "    g = graphviz.Digraph('G',format='png')\n",
    "    g.splines='true'\n",
    "    g.overlap='false'\n",
    "    \n",
    "    if len(edges)>0:\n",
    "        # prepare nodes/edges\n",
    "        if type(edges[0])==str:\n",
    "            # if input params are (box_labels,rel_labels) from SGG\n",
    "            edges1=list()\n",
    "            for i in range(0,len(edges)):\n",
    "                edge = {}\n",
    "                r1,r2,r3 = edges[i].split(' => ')\n",
    "                [r11,r12] = r1.split('_') \n",
    "                edge['node1_id'] = r11 \n",
    "                edge['node1'] = r12\n",
    "                edge['node1_sg'] = True \n",
    "                edge['rel'] = r2 \n",
    "                edge['rel_sg'] = True \n",
    "                [r31,r32] = r3.split('_')\n",
    "                edge['node2_id'] = r31 \n",
    "                edge['node2'] = r32 \n",
    "                edge['node2_sg'] = True \n",
    "                edges1.append(edge)\n",
    "        else:\n",
    "            # if input params are (nodes,edges) from CSKG \n",
    "            edges1 = edges\n",
    "\n",
    "        # build the graph\n",
    "        for i in range(0,len(edges1)):\n",
    "            e = edges1[i]\n",
    "            if e['node1_sg'] == True:\n",
    "                g.node(e['node1_id'],label=e['node1'],shape='rect',style='filled',fillcolor='black',fontcolor='white')\n",
    "            else:\n",
    "                g.node(e['node1_id'],label=e['node1'],shape='ellipse')\n",
    "            if e['node2_sg'] == True:\n",
    "                g.node(e['node2_id'],label=e['node2'],shape='rect',style='filled',fillcolor='black',fontcolor='white')\n",
    "            else:\n",
    "                g.node(e['node2_id'],label=e['node2'],shape='ellipse')\n",
    "            if e['rel_sg'] == True:\n",
    "                g.edge(e['node1_id'],e['node2_id'],label=e['rel'])\n",
    "            else:\n",
    "                g.edge(e['node1_id'],e['node2_id'],label=e['rel'],dir='none') # (CSKG undirected edge)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Objects/Visual Concepts to CSKG format, and Objects Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU Calculation\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Node Similarity based on CSKG Embeddings\n",
    "def node_similarity(node1,node2):\n",
    "    \n",
    "    try:\n",
    "        node1_emb=graph_embeddings_comp[graph_vocab.word_to_idx[node1]]\n",
    "    except:\n",
    "        try:\n",
    "            [node1,x] = node1.split('_')\n",
    "            node1_emb=graph_embeddings_comp[graph_vocab.word_to_idx[node1]]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    try:\n",
    "        node2_emb=graph_embeddings_comp[graph_vocab.word_to_idx[node2]]\n",
    "    except:\n",
    "        try:\n",
    "            [node2,x] = node2.split('_')\n",
    "            node2_emb=graph_embeddings_comp[graph_vocab.word_to_idx[node2]]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    return cosine_similarity([node1_emb],[node2_emb])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parsing to CSKG format\n",
    "def parse_to_cskg(box_labels,rel_labels):\n",
    "    nodes = list()\n",
    "    edges = list()\n",
    "    for i in range(0,len(box_labels)):\n",
    "        node={}\n",
    "        node['id'] = str(i)\n",
    "        node['label'] = '/c/en/' + box_labels[i].replace(' ','_')\n",
    "        node['sg'] = True\n",
    "        nodes.append(node)\n",
    "    for i in range(0,len(rel_labels)):\n",
    "        edge = {}\n",
    "        rel_node = {}\n",
    "        r1,r2,r3 = rel_labels[i].split(' => ')\n",
    "        [r11,r12] = r1.split('_') \n",
    "        node1_id = r11 \n",
    "        node1 = '/c/en/'+r12.replace(' ','_')\n",
    "        rel = '/r/'+r2.replace(' ','_')\n",
    "        [r31,r32] = r3.split('_')\n",
    "        node2_id = r31 \n",
    "        node2 = '/c/en/'+r32.replace(' ','_') \n",
    "        edge['node1']=node1\n",
    "        edge['node1_id']=node1_id\n",
    "        edge['node1_sg']=True\n",
    "        edge['rel']=rel\n",
    "        edge['rel_sg']=True\n",
    "        edge['node2']=node2\n",
    "        edge['node2_id']=node2_id\n",
    "        edge['node2_sg']=True\n",
    "        edges.append(edge)\n",
    "    return nodes,edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects Refinement based on IoU and Node Similarity\n",
    "import copy\n",
    "\n",
    "def refine_objects_list(box_labels,boxes):\n",
    "    # first identify objects to remove based on graph embedding and location\n",
    "    # in case of similarity, the 2nd object is noted for removal from the list \n",
    "    # noted objects are removed\n",
    "    refined_box_labels = box_labels.copy()\n",
    "    idx = list()\n",
    "    for i in range(0,len(refined_box_labels)-1):\n",
    "        obj_node1 = '/c/en/' + refined_box_labels[i].replace(' ','_')\n",
    "        obj_node1_box = boxes[i]\n",
    "        for j in range(i+1,len(refined_box_labels)):\n",
    "            obj_node2 = '/c/en/' + refined_box_labels[j].replace(' ','_')\n",
    "            obj_node2_box = boxes[j]\n",
    "            if node_similarity(obj_node1,obj_node2)>0.5 or bb_intersection_over_union(obj_node1_box,obj_node2_box)>0.7:\n",
    "                idx.append(j)\n",
    "    idx = list(set(idx)) # sort the list and remove duplicates\n",
    "    for i in range(0,len(idx)):\n",
    "        # remove ith object\n",
    "        del refined_box_labels[idx[i]-i]\n",
    "    return refined_box_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def node_to_sg(node):\n",
    "    '''Reverse Parse (CSKG to SG)'''\n",
    "    node1 = node.copy()\n",
    "    if '/c/en/' in node1['label']:\n",
    "        x = node1['label'].split('/')\n",
    "        node1_label = x[3]\n",
    "        obj_sim_scores = np.zeros((len(vg_unique_objs),1))\n",
    "        if node1['sg'] == True:\n",
    "            node1['label'] = node1_label.replace('_',' ')\n",
    "        else:\n",
    "            for i in range(0,len(vg_unique_objs)):\n",
    "                obj_sim_scores[i]=node_similarity('/c/en/'+node1_label,'/c/en/'+vg_unique_objs[i].replace(' ','_'))\n",
    "            if max(obj_sim_scores)>0.5:\n",
    "                max_index = [n==max(obj_sim_scores) for n in obj_sim_scores].index(True)\n",
    "                node1['label'] = vg_unique_objs[max_index]\n",
    "            else:\n",
    "                node1 = None\n",
    "    else:\n",
    "        node1 = None\n",
    "    return node1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Retrieval, SG Construction and Adjustment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "vg_rels_json = json.load(open('../relationships.json'))\n",
    "vg_dict = json.load(open('../VG-SGG-dicts-with-attri.json'))\n",
    "vg_unique_objs = list(dict.fromkeys(vg_dict['object_count'].keys())) # 150 unique rels in VG\n",
    "vg_unique_rels = list(dict.fromkeys(vg_dict['predicate_count'].keys())) # 50 unique rels in VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "\n",
    "rels_dict = vg_dict['predicate_count']\n",
    "rels_dict = {k: v for k, v in sorted(rels_dict.items(), reverse=True, key=lambda item: item[1])}\n",
    "rels_dict = {k: 100*(v/sum(rels_dict.values())) for k, v in rels_dict.items()}\n",
    "# fig = plt.figure(figsize=[100,70])\n",
    "# plt.rcParams[\"font.size\"] = 100\n",
    "# plt.xticks(fontsize = 50, rotation = 60)\n",
    "# plt.xlabel('Predicate')\n",
    "# plt.ylabel('Frequency (%)')\n",
    "# plt.bar(range(len(rels_dict)), rels_dict.values(), tick_label=list(rels_dict.keys()))\n",
    "# plt.savefig(\"freqplot_rels.png\")\n",
    "\n",
    "#objs_dict = vg_dict['object_count']\n",
    "# objs_dict = {k: v for k, v in sorted(objs_dict.items(), reverse=True, key=lambda item: item[1])}\n",
    "# objs_dict = {k: 100*(v/sum(objs_dict.values())) for k, v in objs_dict.items()}\n",
    "# fig2 = plt.figure(figsize=[140,60])\n",
    "# plt.rcParams[\"font.size\"] = 100\n",
    "# plt.xticks(fontsize = 50, rotation = 90)\n",
    "# plt.xlabel('Object')\n",
    "# plt.ylabel('Frequency (%)')\n",
    "# plt.bar(range(len(objs_dict)), objs_dict.values(), tick_label=list(objs_dict.keys()))\n",
    "# plt.savefig(\"freqplot_objs.png\")\n",
    "\n",
    "l = list()\n",
    "l.append(rels_dict)\n",
    "with open('vg_rels_freq.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = rels_dict.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment=None\n",
    "import time\n",
    "def extract_from_cskg(nodes):\n",
    "    new_nodes_rels = list()\n",
    "    \n",
    "    # 1/2 Rels between each pair of nodes\n",
    "    for node1 in nodes:\n",
    "        for node2 in nodes:\n",
    "            t = time.time()\n",
    "            \n",
    "            # (n1,-,n2)\n",
    "            if node1['id']!=node2['id']: # check if both are not the same node\n",
    "                #command = \"$kypher -i $NKG \\\n",
    "                #            --match '(n1)-[]->(n2)' \\\n",
    "                #            --where '(n1 = \\\"\" + node1['label'] + \"\\\") and (n2 = \\\"\" + node2['label'] + \"\\\")'\"\n",
    "                command = \"$kypher -i $NKG \\\n",
    "                            --match '(n1)-[r]->(n2)' \\\n",
    "                            --where 'n1 =~ \\\".*/\" + node_to_sg(node1)['label'] + \"/.*\\\"  and n2 =~ \\\".*/\" + node_to_sg(node2)['label'] + \"/.*\\\"'\"\n",
    "                stats=shell_df(command, shell=True, sep='\\t')\n",
    "                stats.drop(stats[stats['label']=='label'].index, inplace=True)\n",
    "                # skip the ones that have same node on both sides (e.g. man-similarTo-man)\n",
    "                stats.drop(stats[stats['node1']==stats['node2']].index, inplace=True)\n",
    "                # calculate similarity of nodes in each row\n",
    "                stats['similarity']=np.zeros((len(stats),1))\n",
    "                for i in stats.index:\n",
    "                    stats['similarity'][i] = node_similarity(stats['node1'][i],stats['node2'][i])\n",
    "                # remove duplicate rows\n",
    "                stats.drop_duplicates()\n",
    "                # sort stats in descending order by similarity score\n",
    "                stats.sort_values(by=['similarity'], inplace=True, ascending=False)\n",
    "                rels = list(pd.unique(stats['label']))\n",
    "                for rel in rels:\n",
    "                    # one instance of each unique rel\n",
    "                    sub_stats = stats[stats['label']==rel].head(1)\n",
    "                    for i in sub_stats.index:\n",
    "                        if sub_stats.loc[i]['similarity']>=0.5:\n",
    "                            new_nodes_rels.append([sub_stats.loc[i]['node1'],sub_stats.loc[i]['label'],\n",
    "                                                    sub_stats.loc[i]['node2'],sub_stats.loc[i]['similarity']])\n",
    "            # (n2,-,n1)\n",
    "            if node1['id']!=node2['id']: # check if both are not the same node\n",
    "                #command = \"$kypher -i $NKG \\\n",
    "                #            --match '(n1)-[]->(n2)' \\\n",
    "                #            --where '(n1 = \\\"\" + node2['label'] + \"\\\") and (n2 = \\\"\" + node1['label'] + \"\\\")'\"\n",
    "                command = \"$kypher -i $NKG \\\n",
    "                            --match '(n1)-[r]->(n2)' \\\n",
    "                            --where 'n1 =~ \\\".*/\" + node_to_sg(node2)['label'] + \"/.*\\\"  and n2 =~ \\\".*/\" + node_to_sg(node1)['label'] + \"/.*\\\"'\"\n",
    "                stats=shell_df(command, shell=True, sep='\\t')\n",
    "                stats.drop(stats[stats['label']=='label'].index, inplace=True)\n",
    "                # skip the ones that have same node on both sides (e.g. man-similarTo-man)\n",
    "                stats.drop(stats[stats['node1']==stats['node2']].index, inplace=True)\n",
    "                # calculate similarity of nodes in each row\n",
    "                stats['similarity']=np.zeros((len(stats),1))\n",
    "                for i in stats.index:\n",
    "                    stats['similarity'][i] = node_similarity(stats['node1'][i],stats['node2'][i])\n",
    "                # remove duplicate rows\n",
    "                stats.drop_duplicates()\n",
    "                # sort stats in descending order by similarity score\n",
    "                stats.sort_values(by=['similarity'], inplace=True, ascending=False)\n",
    "                rels = list(pd.unique(stats['label']))\n",
    "                for rel in rels:\n",
    "                    # one instance of each unique rel\n",
    "                    sub_stats = stats[stats['label']==rel].head(1)\n",
    "                    for i in sub_stats.index:\n",
    "                        if sub_stats.loc[i]['similarity']>=0.5:\n",
    "                            new_nodes_rels.append([sub_stats.loc[i]['node1'],sub_stats.loc[i]['label'],\n",
    "                                                    sub_stats.loc[i]['node2'],sub_stats.loc[i]['similarity']])\n",
    "            \n",
    "            print('Extraction from CSKG done for '+node1['label']+' and '+node2['label']+' in '+ str(int(time.time()-t)) +' sec)')\n",
    "                \n",
    "            \n",
    "    # 2/2 Rels of each node      \n",
    "    for node in nodes:\n",
    "        t = time.time()\n",
    "        \n",
    "        # Part 1/2: (node, --, --)\n",
    "        command = \"$kypher -i $NKG \\\n",
    "                    --match '(n1)-[]->()' \\\n",
    "                    --where '(n1 = \\\"\" + node['label'] + \"\\\")'\"\n",
    "        #command = \"$kypher -i $NKG \\\n",
    "        #            --match '(n1)-[r]->(n2), (r)-[:source]->(source:`\\\"VG\\\"`)' \\\n",
    "        #            --where 'n1 =~ \\\".*/woman/.*\\\"  and n2 =~ \\\".*/racket/.*\\\"'\"\n",
    "        stats=shell_df(command, shell=True, sep='\\t')\n",
    "        stats.drop(stats[stats['label']=='label'].index, inplace=True)\n",
    "        # skip the ones that have same node on both sides (e.g. man-similarTo-man)\n",
    "        stats.drop(stats[stats['node1']==stats['node2']].index, inplace=True)\n",
    "        # calculate similarity of nodes in each row\n",
    "        stats['similarity']=np.zeros((len(stats),1))\n",
    "        for i in stats.index:\n",
    "            stats['similarity'][i] = node_similarity(stats['node1'][i],stats['node2'][i])\n",
    "        # remove duplicate rows\n",
    "        stats.drop_duplicates()\n",
    "        # sort stats in descending order by similarity score\n",
    "        stats.sort_values(by=['similarity'], inplace=True, ascending=False)\n",
    "        rels = list(pd.unique(stats['label']))\n",
    "        for rel in rels:\n",
    "            # one instance of each unique rel\n",
    "            sub_stats = stats[stats['label']==rel].head(1)\n",
    "            for i in sub_stats.index:\n",
    "                if sub_stats.loc[i]['similarity']>=0.5:\n",
    "                    new_nodes_rels.append([sub_stats.loc[i]['node1'],sub_stats.loc[i]['label'],\n",
    "                                            sub_stats.loc[i]['node2'],sub_stats.loc[i]['similarity']])\n",
    "\n",
    "        # Part 2/2: (--, --, node)\n",
    "        command = \"$kypher -i $NKG \\\n",
    "                    --match '()-[]->(n2)' \\\n",
    "                    --where '(n2 = \\\"\" + node['label'] + \"\\\")'\"\n",
    "        stats=shell_df(command, shell=True, sep='\\t')\n",
    "        stats.drop(stats[stats['label']=='label'].index, inplace=True)\n",
    "        # skip the ones that have same node on both sides (e.g. man-similarTo-man)\n",
    "        stats.drop(stats[stats['node1']==stats['node2']].index, inplace=True)\n",
    "        # calculate similarity of nodes in each row\n",
    "        stats['similarity']=np.zeros((len(stats),1))\n",
    "        for i in stats.index:\n",
    "            stats['similarity'][i] = node_similarity(stats['node1'][i],stats['node2'][i])\n",
    "        # remove duplicate rows\n",
    "        stats.drop_duplicates()\n",
    "        # sort stats in descending order by similarity score\n",
    "        stats.sort_values(by=['similarity'], inplace=True, ascending=False)\n",
    "        rels = list(pd.unique(stats['label']))\n",
    "        for rel in rels:\n",
    "            # one instance of each unique rel;\n",
    "            sub_stats = stats[stats['label']==rel].head(1)\n",
    "            for i in sub_stats.index:\n",
    "                if sub_stats.loc[i]['similarity']>=0.5:\n",
    "                    new_nodes_rels.append([sub_stats.loc[i]['node1'],sub_stats.loc[i]['label'],\n",
    "                                            sub_stats.loc[i]['node2'],sub_stats.loc[i]['similarity']])\n",
    "\n",
    "        print('Extraction from CSKG done for '+node['label']+' in '+ str(int(time.time()-t)) +' sec)')\n",
    "    return new_nodes_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zs_rel_retrieval(nodes):\n",
    "\n",
    "    # extract and add relevant nodes and edges from CSKG\n",
    "    new_nodes_rels = extract_from_cskg(nodes)\n",
    "\n",
    "    new_nodes=list()\n",
    "    new_edges=list()\n",
    "    for new_node_rel in new_nodes_rels:\n",
    "        new_node1 = new_node_rel[0]\n",
    "        new_rel = new_node_rel[1]\n",
    "        new_node2 = new_node_rel[2]\n",
    "\n",
    "        # check if the 1st new node is already present\n",
    "        if (new_node1 in [n['label'] for n in nodes+new_nodes]):\n",
    "            # if yes, link the new edge to the existing node\n",
    "            for n in nodes+new_nodes:\n",
    "                if n['label']==new_node1:\n",
    "                    new_node1_id = n['id']\n",
    "                    new_node1_sg = n['sg']\n",
    "        else:\n",
    "            # otherwise create new node and then link\n",
    "            new_node1_id = str(int(nodes[-1]['id'])+len(new_nodes)+1)\n",
    "            new_node1_sg = False\n",
    "            new_nodes.append({'id':new_node1_id,\n",
    "                              'label':new_node1,\n",
    "                              'sg':new_node1_sg})\n",
    "\n",
    "        # similarly for 2nd new node\n",
    "        if (new_node2 in [n['label'] for n in nodes+new_nodes]):\n",
    "            for n in nodes+new_nodes:\n",
    "                if n['label']==new_node2:\n",
    "                    new_node2_id = n['id']\n",
    "                    new_node2_sg = n['sg']\n",
    "        else:                \n",
    "            new_node2_id = str(int(nodes[-1]['id'])+len(new_nodes)+1)\n",
    "            new_node2_sg = False\n",
    "            new_nodes.append({'id':new_node2_id,\n",
    "                              'label':new_node2,\n",
    "                              'sg':new_node2_sg})\n",
    "\n",
    "        new_edge = {'node1': new_node1,\n",
    "                    'node1_id': new_node1_id,\n",
    "                    'node1_sg': new_node1_sg,\n",
    "                    'rel': new_rel,\n",
    "                    'rel_sg': False,\n",
    "                    'node2': new_node2,\n",
    "                    'node2_id': new_node2_id,\n",
    "                    'node2_sg':new_node2_sg}\n",
    "        new_edges.append(new_edge)\n",
    "\n",
    "    nodes1 = nodes + new_nodes\n",
    "    #edges1 = edges + new_edges\n",
    "    return nodes1,new_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment to SG format for downstream task or evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('../CSKGtoSG-rels.xlsx')\n",
    "def parse_to_sg(nodes, edges, vg_unique_objs=vg_unique_objs, vg_unique_rels=vg_unique_rels):\n",
    "    nodes1=list()\n",
    "    edges1=list()\n",
    "    for node in nodes:\n",
    "        node = node_to_sg(node)\n",
    "        if (node is not None) and (node['label'] not in [n['label'] for n in nodes1]):\n",
    "            nodes1.append(node)\n",
    "    for edge in edges:\n",
    "        node1 = node_to_sg({'id': edge['node1_id'], 'label': edge['node1'], 'sg': edge['node1_sg']})\n",
    "        node2 = node_to_sg({'id': edge['node2_id'], 'label': edge['node2'], 'sg': edge['node2_sg']})\n",
    "        rel = None\n",
    "        # if both nodes exist\n",
    "        if (node1 is not None) and (node2 is not None):\n",
    "            # if node1 or node2 already present, link to it\n",
    "            if node1['label'] in [node['label'] for node in nodes1]:\n",
    "                for node in nodes1:\n",
    "                    if node['label']==node1['label']:\n",
    "                        node1['id'] = node['id']\n",
    "                        node1['sg'] = node['sg']\n",
    "            if node2['label'] in [node['label'] for node in nodes1]:\n",
    "                for node in nodes1:\n",
    "                    if node['label']==node2['label']:\n",
    "                        node2['id'] = node['id']\n",
    "                        node2['sg'] = node['sg']\n",
    "            # process rel if different nodes on both sides\n",
    "            if node1['id']!=node2['id']:\n",
    "                try:\n",
    "                    x = edge['rel'].split('/')\n",
    "                    x = x[2].replace('_',' ')\n",
    "                    if x in vg_unique_rels:\n",
    "                        rel = x\n",
    "                    else:    \n",
    "                        for i in df.index:\n",
    "                            if df['cskg_rel'][i].lower() == edge['rel'].lower():\n",
    "                                rel = df['sg_rel'].iloc[i]\n",
    "                    if (rel is not None) and str(rel)!='nan':\n",
    "                        edges1.append({\n",
    "                            'node1': node1['label'],\n",
    "                            'node1_id': node1['id'],\n",
    "                            'node1_sg': node1['sg'],\n",
    "                            'rel': rel,\n",
    "                            'rel_sg': edge['rel_sg'],\n",
    "                            'node2': node2['label'],\n",
    "                            'node2_id': node2['id'],\n",
    "                            'node2_sg': node2['sg']\n",
    "                        })\n",
    "                except:\n",
    "                    pass\n",
    "    # discard nodes that are not used in edges1\n",
    "    nodes1 = [n for n in nodes1 if n['id'] in list(set([e['node1_id'] for e in edges1]+[e['node2_id'] for e in edges1]))]\n",
    "    return nodes1, edges1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Zero Shot SGs to dir\n",
    "def save_sg_format(img_path, nodes, edges):\n",
    "    custom_prediction_file = open(f'{dir_eval_io}1_det_objs/'+img_path.split('0_images/')[1]+'/custom_prediction.json')\n",
    "    custom_prediction = json.load(custom_prediction_file)\n",
    "    custom_data_info_file = open(f'{dir_eval_io}1_det_objs/'+img_path.split('0_images/')[1]+'/custom_data_info.json')\n",
    "    custom_data_info = json.load(custom_data_info_file)\n",
    "    ind_to_classes = custom_data_info['ind_to_classes']\n",
    "    ind_to_predicates = custom_data_info['ind_to_predicates']\n",
    "    for node in nodes:\n",
    "        if node['sg']==False:\n",
    "            bbox = custom_prediction['0']['bbox']\n",
    "            bbox.append(bbox[1])\n",
    "            custom_prediction['0']['bbox'] = bbox\n",
    "            bbox_labels = custom_prediction['0']['bbox_labels']\n",
    "            bbox_labels.append(ind_to_classes.index(node['label']))\n",
    "            custom_prediction['0']['bbox_labels'] = bbox_labels\n",
    "            bbox_scores = custom_prediction['0']['bbox_scores']\n",
    "            bbox_scores.append(max(bbox_scores))\n",
    "            custom_prediction['0']['bbox_scores'] = bbox_scores\n",
    "    for edge in edges:\n",
    "        if edge['rel_sg']==False:\n",
    "            bbox_labels = custom_prediction['0']['bbox_labels']\n",
    "            node1 = bbox_labels.index(ind_to_classes.index(edge['node1']))\n",
    "            node2 = bbox_labels.index(ind_to_classes.index(edge['node2']))\n",
    "            rel_pairs = custom_prediction['0']['rel_pairs']\n",
    "            rel_pairs.insert(0,[node1,node2])\n",
    "            custom_prediction['0']['rel_pairs'] = rel_pairs\n",
    "            rel_labels = custom_prediction['0']['rel_labels']\n",
    "            rel_labels.insert(0,ind_to_predicates.index(edge['rel']))\n",
    "            custom_prediction['0']['rel_labels'] = rel_labels\n",
    "            rel_scores = custom_prediction['0']['rel_scores']\n",
    "            rel_scores.insert(0,max(rel_scores))\n",
    "            custom_prediction['0']['rel_scores'] = rel_scores\n",
    "            rel_all_scores = custom_prediction['0']['rel_all_scores']\n",
    "            rel_all_scores.insert(0,rel_all_scores[1])\n",
    "            custom_prediction['0']['rel_all_scores'] = rel_all_scores\n",
    "    os.mkdir(f'{dir_eval_io}2_zs_sg/'+img_path.split('0_images/')[1])\n",
    "    with open(f'{dir_eval_io}2_zs_sg/'+img_path.split('0_images/')[1]+'/custom_prediction.json', 'w') as f:\n",
    "        json.dump(custom_prediction, f)\n",
    "    with open(f'{dir_eval_io}2_zs_sg/'+img_path.split('0_images/')[1]+'/custom_data_info.json', 'w') as f:\n",
    "        json.dump(custom_data_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together - running it over the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "for filename in os.listdir(f'{dir_eval_io}0_images/'):\n",
    "    img_path = f'{dir_eval_io}0_images/{filename}'\n",
    "    \n",
    "    if os.path.exists(f'{dir_eval_io}graph_figures/'+img_path.split('0_images/')[1]+'/06_zeroshotsg'):\n",
    "        #print('***** EXISTS: '+ img_path +' *****')\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        #print('***** PROCESSING '+ img_path +' *****')\n",
    "        \n",
    "        # Load detected objects in images\n",
    "        [box_labels,rel_labels,boxes] = load_sg(img_path, 10) # 2nd arg is max objects\n",
    "        \n",
    "        # Parse to CSKG\n",
    "        [nodes,[]] = parse_to_cskg(box_labels,[])\n",
    "        \n",
    "        # Refine objects based on IoU and node similarity\n",
    "        box_labels = refine_objects_list(box_labels,boxes)\n",
    "        \n",
    "        # Retrieve Rels and Gen scene graph (zero shot retrieval) \n",
    "        [nodes1,edges1] = zs_rel_retrieval(nodes)\n",
    "        g3 = draw_graph(nodes1,edges1)\n",
    "        g3.render(f'{dir_eval_io}graph_figures/'+img_path.split('0_images/')[1]+'/06_zeroshotsg')\n",
    "\n",
    "        # Adjust to SG\n",
    "        [nodes3,edges3] = parse_to_sg(nodes1,edges1)\n",
    "        g4 = draw_graph(nodes3,edges3)\n",
    "        g4.render(f'{dir_eval_io}graph_figures/'+img_path.split('0_images/')[1]+'/07_zeroshotsg_adjusted')\n",
    "\n",
    "        # Save final graph as JSON\n",
    "        graph = {'nodes':nodes3, 'edges':edges3}\n",
    "        with open(f'{dir_eval_io}graph_figures/'+img_path.split('0_images/')[1]+'.json', 'w') as f:\n",
    "            json.dump(graph, f)\n",
    "\n",
    "        # Save the final result in SGG format\n",
    "        save_sg_format(img_path, nodes3, edges3)\n",
    "print('.. done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Groundtruth for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(f'{dir_eval_io}0_images'):\n",
    "    img_path = f'{dir_eval_io}/0_images/{filename}'\n",
    "    img_id = int(img_path.split('0_images/')[1].split('.jpg')[0])\n",
    "    box_labels = list()\n",
    "    rel_labels = list()\n",
    "    if os.path.exists(f'{dir_eval_io}0_gt_scene_graphs/'+img_path.split('0_images/')[1]+'_gt_vg'):\n",
    "        #print('***** EXISTS: '+ img_path +' *****')\n",
    "        continue\n",
    "    else:\n",
    "        #print('***** PROCESSING '+ img_path +' *****')\n",
    "        for img in vg_rels_json:\n",
    "            if img['image_id'] == img_id:\n",
    "                for rel in img['relationships']:\n",
    "                    if rel['predicate'].lower() in vg_unique_rels:\n",
    "                        # in relationships.json, there is either 'name' or 'names' key for subject and object\n",
    "                        pred = rel['predicate'].lower()\n",
    "                        if 'name' in rel['subject'].keys():\n",
    "                            if 'name' in rel['object'].keys():\n",
    "                                subj = rel['subject']['name']\n",
    "                                obj = rel['object']['name']\n",
    "                            else:\n",
    "                                subj = rel['subject']['name']\n",
    "                                obj = rel['object']['names'][0]\n",
    "                        else:\n",
    "                            if 'name' in rel['object'].keys():\n",
    "                                subj = rel['subject']['names'][0]\n",
    "                                obj = rel['object']['name']\n",
    "                            else:\n",
    "                                subj = rel['subject']['names'][0]\n",
    "                                obj = rel['object']['names'][0]\n",
    "                        if (subj in vg_unique_objs) and (obj in vg_unique_objs):\n",
    "                            if subj in box_labels:\n",
    "                                subj_id = box_labels.index(subj)\n",
    "                            else:\n",
    "                                box_labels.append(subj)\n",
    "                                subj_id = box_labels.index(subj)\n",
    "                            if obj in box_labels:\n",
    "                                obj_id = box_labels.index(obj)\n",
    "                            else:\n",
    "                                box_labels.append(obj)\n",
    "                                obj_id = box_labels.index(obj)\n",
    "                            rel_labels.append(str(subj_id)+'_'+subj +' => '+ pred +' => '+ str(obj_id)+'_'+obj)\n",
    "        g = draw_graph(box_labels,rel_labels)\n",
    "        g.render(f'{dir_eval_io}0_gt_scene_graphs/'+img_path.split('0_images/')[1]+'_gt_vg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json, os, numpy as np, copy\n",
    "vg_rels_json = json.load(open('ZS_SGG_CSKG/relationships.json'))\n",
    "vg_dict = json.load(open('ZS_SGG_CSKG/VG-SGG-dicts-with-attri.json'))\n",
    "vg_unique_objs = list(dict.fromkeys(vg_dict['object_count'].keys())) # 150 unique rels in VG\n",
    "vg_unique_rels = list(dict.fromkeys(vg_dict['predicate_count'].keys())) # 50 unique rels in VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 done\n",
      "50 done\n",
      "100 done\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "overall_recall = list()\n",
    "for K in [20,50,100]:\n",
    "\n",
    "    pred_sg_dir = f'{dir_eval_io}/2_zs_sg/'\n",
    "    assert(os.listdir(pred_sg_dir) == os.listdir(pred_sg_dir_cs))\n",
    "\n",
    "    recall = list() #zR@K for zero-shot scene graphs\n",
    "\n",
    "    l = os.listdir(pred_sg_dir)\n",
    "    #l[-2:] = []\n",
    "    #l=l[0:5]\n",
    "    for pred_sg in l: #['2317213.jpg']\n",
    "        # 1/3 Retrieve groundtruth predicates\n",
    "        gt_rels = [img['relationships'] for img in vg_rels_json if img['image_id']==int(pred_sg.split('.')[0])][0]\n",
    "        gt_predicates = list()\n",
    "        for gt_rel in gt_rels:\n",
    "            if 'name' in gt_rel['subject'].keys():\n",
    "                obj1 = gt_rel['subject']['name']\n",
    "            else:\n",
    "                obj1 = gt_rel['subject']['names'][0]\n",
    "            if 'name' in gt_rel['object'].keys():\n",
    "                obj2 = gt_rel['object']['name']\n",
    "            else:\n",
    "                obj2 = gt_rel['object']['names'][0]\n",
    "            rel = gt_rel['predicate'].lower()\n",
    "\n",
    "            if rel in vg_unique_rels:\n",
    "                gt_predicates.append(rel)\n",
    "\n",
    "        # Retrieve keys (idnices of obj/pred labels)\n",
    "        custom_data_info = json.load(open(pred_sg_dir+pred_sg+'/custom_data_info.json'))\n",
    "        ind_to_classes = custom_data_info['ind_to_classes']\n",
    "        ind_to_predicates = custom_data_info['ind_to_predicates']\n",
    "\n",
    "        # 2/3 Retrieve predicted predicates (without commonsense)\n",
    "        custom_prediction = json.load(open(pred_sg_dir+pred_sg+'/custom_prediction.json'))\n",
    "        bbox_labels = custom_prediction['0']['bbox_labels']\n",
    "        bbox_scores = custom_prediction['0']['bbox_scores']\n",
    "        rel_pairs = custom_prediction['0']['rel_pairs']\n",
    "        rel_labels = custom_prediction['0']['rel_labels']\n",
    "        rel_scores = custom_prediction['0']['rel_scores']\n",
    "        pred_objects = [ind_to_classes[ind] for ind in bbox_labels]\n",
    "        pred_object_pairs = [[pred_objects[inds[0]],pred_objects[inds[1]]] for inds in rel_pairs]\n",
    "        pred_predicates = [ind_to_predicates[ind] for ind in rel_labels]\n",
    "        assert(len(pred_object_pairs)==len(pred_predicates))\n",
    "        k_pred_predicates = pred_predicates[:min(K,len(pred_predicates))] # Top K Predicted Predicates\n",
    "\n",
    "        # Compute zR@K\n",
    "        count_preds = 0 # number of correct predicted predicates\n",
    "        gt_predicates1=copy.deepcopy(gt_predicates)\n",
    "        for i in range(0,len(k_pred_predicates)):\n",
    "            if k_pred_predicates[i] in gt_predicates1:\n",
    "                count_preds += 1\n",
    "                gt_predicates1.pop(gt_predicates1.index(k_pred_predicates[i]))\n",
    "\n",
    "        if len(gt_predicates)>0:\n",
    "            recall.append(count_preds*100/len(gt_predicates))\n",
    "        else:\n",
    "            #pass\n",
    "            recall.append(0)\n",
    "\n",
    "    print(K, 'done')\n",
    "\n",
    "    overall_recall.append(sum(recall)/len([r for r in recall]))\n",
    "\n",
    "results = {'vg': overall_recall}\n",
    "json.dump(results, open(f'{dir_eval_io}/results.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vg': [13.3343771806422, 21.1792970272041, 25.73342946772114]}\n"
     ]
    }
   ],
   "source": [
    "results = json.load(open(f'{dir_eval_io}/results.json'))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
